* Reinforcement Learning :
  ---------------

   - #agent-learn #interactive-env #incentivise #feedback #trial&Error #rewards
     #Learning-takes-practice

   - maximize reward for given situation 
   - Learn through consequences of actions in an environment to reach a goal.

   In RL, agent learns by trial-error method during episode 
   where it gets feedback in terms of rewards

   Eg Training a Dog
       \
        - behavior that incentivise

   -> In reinforcement learning (RL), an agent is trained to achieve a goal 
      based on the feedback it receives as it interacts with an environment.
    
      It collects a number as a reward for each action it takes. 
      Actions that help the agent achieve its goal are incentivized with higher numbers

      Unhelpful actions result in a low reward or no reward.

   goal => It’s particularly useful for addressing sequential problems with long-term goals

   -> Eg application :

       1) classic video games
       2) Winde Energy Optimization
       3) Industrail Robotics
       4) Fraud Detection 
       5) Autonomous driving

    * Agent :
      -----
       -> The piece of software you are training is called an agent.
       -> It makes decisions in an environment to reach a goal.
       -> In AWS DeepRacer, the agent is the AWS DeepRacer car and 
          its goal is to finish * laps around the track as fast as it can while, in some cases, avoiding obstacles.

    * Environment:
      -------
       -> For AWS DeepRacer, this is a track in our simulator or in real life.


    * state :
      -----
       -> state is defined by the current position within the environment that is visible,
          or known to an agent
          (In DeepRacer case, it is the image captured by the camera)

    * Action :
      -----
       -> For every state, an agent needs to take an action toward achieving its goal.
       -> An AWS DeepRacer car approaching a turn can choose to accelerate or brake and turn left, right, or go straight.

    * Reward :
      -----
       -> Feedback is given to an agent in each state 
          feedback is numerical reward

    * episode :
      -----
       -> period of trial & error, when an agent makes a decision & gets feedback from its environment

       For DeepRacer :-
          episode begins when car leaves the starting pos.
          & ends at the terminal state, when it finishes a lap, bumps into an obstacle, or drive off the track

 => In a reinforcement learning model, an agent learns in an interactive real-time environment 
    by trial and error using feedback from its own actions. 
    Feedback is given in the form of rewards.


-----------
 Approach 
-----------

  * Algorithm :
    ---------
     -> training algo tells the model to gather as many rewards as possible.
         Model Objective : maximize total cumulitive reward

         Deep Racer Strategy :-
            1) SAC - Soft Actor Critic
                   - embraces exploration and is data-efficient, but can lack stability.
            
            2) PPO - proximal policy optimization 
                   - stable but data-hungry

   * Action Space :-
     ---------
      -> An action space is the set of all valid actions, or choices, 
         available to an agent as it interacts with an environment.

        1) Discrete Action Space :
           ---
            -> all of an agent's possible actions for each state in a finite set of steering angle and throttle value combinations.

        2) Continuous Action Space :
           ---
            -> allows the agent to select an action from a range of values that you define for each state.
    
    * HyperParamters :
      ---------
       -> are variables that control the performance of your agent during training.

       Learning Rate :
       -------------
        -> is a hyperparameter that controls how many new experiences are counted in learning at each step.
           - A higher learning rate results in faster training but may reduce the model’s quality.

    * Reward Function :
      ----------
       encourage agent to reach its goal
       - figure how to reward which actions is imp jobs

       It helps agent to determine if action took was bad or good


    * Exploration vs Exploitation :
      -------------
       - tradeoff between exploration & Exploitation

         -> When a car first starts out, it explores by wandering in random directions. 
            However, the more training an agent gets, the more it learns about an environment. 
            This experience helps it become more confident about the actions it chooses.

         -> Exploitation means the car begins to exploit or use information from previous experiences to help it reach its goal.


    * Reward Graph :
      -----------
       -> Plotting the total reward from each episode allows you to see how the model performs over time.

    
    * Model behavior :
      ---------
       Total rewards vs Episodes graph


------------------------------------------------

* AWS DeepRacer 
  ____________

  Evaluation Process in AWS DeepRacer Console
  
  Graphs :-
  ___

  1) Average Reward :
       This graph represents the average reward the agent earns during a training iteration. 
       The average is calculated by averaging the reward earned across all episodes in the training iteration. 
       An episode begins at the starting line and ends when the agent completes one loop around the track 
       or at the place the vehicle left the track or collided with an object. 
       Toggle the switch to hide this data.

  2) Average percentage completion (training)
       The training graph represents the average percentage of the track completed by the agent in all training episodes in the current training. 
       It shows the performance of the vehicle while experience is being gathered.

  3) Average percentage completion (evaluation)
       While the model is being updated, the performance of the existing model is evaluated. 
       The evaluation graph line is the average percentage of the track completed by the agent in all episodes run during the evaluation period.

  4) Best model line
       This line allows you to see which of your model iterations had the highest average progress during the evaluation. 
       The checkpoint for this iteration will be stored. 
       A checkpoint is a snapshot of a model that is captured after each training (policy-updating) iteration.

  5) Reward primary y-axis
      This shows the reward earned during a training iteration.

  6) Percentage track completion secondary y-axis
      This shows you the percentage of the track the agent completed during a training iteration.

  7) Iteration x-axis
      This shows the number of iterations completed during your training job.


  * Avoid overfitting 
    ________________ 

      Overfitting or Overtraining

      AWS DeepRacer 
        -> an issue when a model is trained on a specific track for too long.

        Overtrained
        
        -> An overtrained model, on the other hand, learns to navigate using landmarks specific to an individual track


  * Adjust Hyperparameters
    __________________

      -> The hyperparameters are variables that essentially act as settings for the training algorithm 
         that control the performance of your agent during training.

    Learning Rate :
    ______________ 
      -> learning rate controls how many new experiences are counted in learning at each step.


      swinging & togglig & Learning Rate :
      ---------------
       A lower learning rate makes learning take longer but can help increase the quality of your model.
